---
title: "✰ Dr. PySpark: How I Learned to Stop Worrying and Love Data Pipeline Testing"
date: 2020-12-18
layout:
tags: [spark, data pipeline, python, data engineer]
---
<!--
author: Boostraptheme
author URL: https://boostraptheme.com
License: Creative Commons Attribution 4.0 Unported
License URL: https://creativecommons.org/licenses/by/4.0/
-->

<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="Pyspark pipeline testing with CICD">
    <meta name="author" content="soyel alam">
    <meta name="keywords" content="Spark, Testing, Data Pipeline, Jenkins, CICD, Big Data, Data Engineering">
    <meta name="viewport" content="width=device-width, initial-scale=0.4">

    <link rel="shortcut icon" href="{{ '/img/blog-solid.ico' | relative_url }}">
    <title>Dr. PySpark: How I Learned to Stop Worrying and Love Data Pipeline Testing</title>

    <!-- Global stylesheets -->
    <link href="{{ '/css/bootstrap/bootstrap.min.css' | relative_url }}" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <!-- <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet"> -->
    <script src="https://kit.fontawesome.com/004e4d6853.js" crossorigin="anonymous"></script>
    <link href="{{ '/css/devicons/css/devicons.min.css' | relative_url }}" rel="stylesheet">
    <link href="{{ '/css/simple-line-icons/css/simple-line-icons.css' | relative_url }}" rel="stylesheet">
    <link href="{{ '/css/style.css' | relative_url }}" rel="stylesheet">
    <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
</head>

<body id="page-top">

    <nav class="navbar navbar-expand-lg navbar-dark bg-blog fixed-top" id="sideNav">
      <a class="navbar-brand " href="#page-top">
        <!-- <span class="d-block d-lg-none  mx-0 px-0"><img src="{{ '/img/logo-white.png' | relative_url }}" alt="" class="img-fluid"></span> -->
        <span class="d-none d-lg-block">
          <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="{{ '/img/profile.png' | relative_url }}" alt="">
        </span>
      </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <ul class="navbar-nav">
                <li class="nav-item">
                    <a class="nav-link " href="{{ '/index.html' | relative_url }}#about">About</a>
                </li>
            </li>
            <li class="nav-item">
                <a class="nav-link  active" href="{{ '/index.html' | relative_url }}#portfolio">Blog</a>
            </li>
                <li class="nav-item">
                    <a class="nav-link " href="{{ '/index.html' | relative_url }}#experience">Experience</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link " href="i../ndex.html#skills">Skills</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link " href="{{ '/index.html' | relative_url }}#awards">Education</a>
                <li class="nav-item">
                    <a class="nav-link " href="{{ '/index.html' | relative_url }}#contact">Contact</a>
                </li>
                <li>
                    <ul class="list-inline social-icon-f top-data">
                      <li><a href="https://www.linkedin.com/in/soyel-alam-98945598/" target="_empty"><i class="fa top-social fa-linkedin" style="color: #4AB3F4; border-color:#4AB3F4;"></i></a></li>
                      <li><a href="https://www.github.com/soyelherein" target="_empty"><i class="fa top-social fa-github" style="color:#101110; border-color:#101110;"></i></a></li>
                      <li><a href="https://twitter.com/Soyelherein" target="_empty"><i class="fa top-social fa-twitter" style="color: #4AB3F4; border-color:#4AB3F4;"></i></a></li>
                      <li><a href="https://www.medium.com/@soyelherein" target="_empty"><i class="fa top-social fa-medium" style="color: #101110; border-color:#101110;"></i></a></li>
                    </ul>
                  </li>
            </ul>
        </div>
    </nav>

    <div class="container-fluid p-0">

    <!--====================================================
                        PORTFOLIO
    ======================================================-->
    <section class="resume-section p-3 p-lg-5 d-flex d-column" id="blog">
        <div class="my-auto">
            <div class="mb-0">
                  <h2>Dr. PySpark: How I Learned to Stop Worrying and Love Data Pipeline Testing</h2>
                        <div class="container">
                            <div class="row">
                                <div class="col-md-6 col-sm-6">
                                    <p>
                                        One major challenge in the data pipeline is to reliably test the pipeline codes.
                                        This is because the outcome of the logic is tightly coupled with data.<br>
                                        One way to overcome this is to use immutable data to test the pipeline.
                                        Obviously, this requires a good knowledge of the application and how well the data matches business requirements.<br>
                                        This also requires some set-ups to enable the developer to focus on building the application logic.
                                        In this blog-post, we will discuss a pluggable approach, that provides a testing environment based on locally stored files.
                                        Enabling developers to follow test-driven development, identify early bugs, and release the code via CICD based on the test outcome.
                                       </p>
                                </div>
                            <div class="col-md-6 col-sm-6">
                        <!-- <img src="{{ '/img/pipeline.png' | relative_url }}" style="max-width: 100%;" /> -->
                        <img src="{{ '/img/blog' | relative_url }}_pyspark_cicd/bear.gif" width="480" height="270" frameBorder="0" class="giphy-embed" alt="Testing breaks"/>
                                   </div></div></div>
                  <h3>Introduction</h3>
                  <p>
                      We will use <a href='https://spark.apache.org/'>Apache PySpark</a> for developing our pipeline. <a href='https://docs.pytest.org/en/stable/'>Pytest</a> will be used for test environment preparation and testing code.
                      Follow along code is available at <a href='https://github.com/soyelherein/pyspark-cicd-template'><strong>https://github.com/soyelherein/pyspark-cicd-template</strong></a><br/><br/>
                  </p>
                  <p>
                    <!-- <div class="container">
                        <div class="row">
                        <div class="col-md-8 col-sm-8">
                               </div></div></div></p>
                  <p> -->
                    Let’s consider we have a pipeline that consumes the “pageviews” file and merges it into the final target table.
                      <div class="container">
                        <div class="row">
                            <div class="col-md-6 col-sm-6">
                                <div class="col-md-4 col-sm-4"><strong>Input Table</strong></div>
                                <table border="1px solid black">
                                    <tr>
                                        <th>email</th><th>page_view</th><th>created_date</th><th>last_active</th>
                                    </tr>
                                    <tr>
                                        <td>james@example.com</td><td>10</td><td>2020-01-01</td><td>2020-07-04</td>
                                    </tr>
                                    <tr>
                                        <td>mary@example.com</td><td>100</td><td>2020-02-04</td><td>2020-02-04</td>
                                    </tr>
                                    <tr>
                                        <td>john@example.com</td><td>1</td><td>2020-03-04</td><td>2020-06-04</td>
                                    </tr>
                                </table>
                            </div>
                            <div class="col-md-1 col-sm-1"></div>
                            <div class="col-md-5 col-sm-5">
                                <div class="col-md-5 col-sm-5"><strong>Incremental File</strong></div>
                                <table border="1px solid black">
                                    <tr>
                                        <th>email,pages</th>
                                    </tr>
                                    <tr>
                                        <td>james@example.com,home</td>
                                    </tr>
                                    <tr>
                                        <td>james@example.com,about</td>
                                    </tr>
                                    <tr>
                                        <td>patricia@example.com,home</td>
                                    </tr>
                                </table>
                            </div>
                        </div>
                        <div class="row" style="border: 10px,10px,10px,10px;">
                            <div class="col-md-5 col-sm-5" style="border: 10px,10px,10px,10px;">
                                <div class="col-md-4 col-sm-4"><strong>Output Table</strong></div>
                                <table border="1px solid black">
                                    <tr>
                                        <th>email</th><th>page_view</th><th>created_date</th><th>last_active</th>
                                    </tr>
                                    <tr>
                                        <td>james@example.com</td><td>12</td><td>2020-01-01</td><td>2020-07-21</td>
                                    </tr>
                                    <tr>
                                        <td>mary@example.com</td><td>100</td><td>2020-02-04</td><td>2020-02-04</td>
                                    </tr>
                                    <tr>
                                        <td>john@example.com</td><td>1</td><td>2020-03-04</td><td>2020-06-04</td>
                                    </tr>
                                    <tr>
                                        <td>patricia@example.com</td><td>1</td><td>2020-07-21</td><td>2020-07-01</td>
                                    </tr>
                                </table>
                            </div>
                        </div>
                    </div>
                    <br>
                    <script src="https://gist.github.com/soyelherein/0297e66134f48025aa536d3af587e7cb.js"></script>
                    We will require tables and files for development. They will be made available at runtime by fixture for Pytest by reading locally stored CSV files. 
                    In Pytest we can define fixture functions that will be common accross tests in a <a href="https://docs.pytest.org/en/stable/fixture.html#conftest-py-sharing-fixture-functions">conftest.py</a>.
                    We have defined a method  <a href="https://github.com/soyelherein/pyspark-cicd-template/blob/286111c75714d09092df68d92c57012f3e5fc272/tests/conftest.py#L90">setup_testbed</a>, 
                    for producing the DataFrames and tables based on a JSON configurations file test_bed.json. Below is the content of the config JSON file:
                    <script src="https://gist.github.com/soyelherein/2ada1d422625dbed3f6da22330dccded.js"></script> 
                    Now, we know we have the code for providing a runtime test environment, we can just place a few sample files and start incrementally test and develop our pipeline.
                    We will discuss more on this in the Testbed section.
                    Let's modularizing our application into testable units so that it can be fitted into the frame.
                    </p>
                    <h3>Decouple Application</h3>
                    Our overall project structure would look like below:
                    <script src="https://gist.github.com/soyelherein/4846e622ee40adcd2b2680aac5fbb986.js"></script>
                    <p>There are typically five major sections in the pipeline code.
                        <ol><strong>
                            <li>Spark session management</li>
                            <li>Declarations for static configutarion</li>
                            <li>Extract</li>
                            <li>Transform</li>
                            <li>Load</li>
                          </strong></ol>
                        <strong>jobs</strong> — We design separate Extract and Load methods to handle the IO operations of reading files and writing into table.
                         We will test their behavior using <a href="https://docs.python.org/3/library/unittest.mock.html">mocks</a>. <br>
                         Transform method deals with the business logic, modify it to take DataFrames as input and returns DataFrames as output, so that it can compared against the locally stored gold standard data.
                         Additionally, we will have an entry point method named run for our pipeline that does the coordination between other methods.  </p>
                        <blockquote><p>Extract — Reads the incremental file and historical data from the table and return 2 Dataframes</p></blockquote>
                        <blockquote><p>Transform — Calculates the metrics based on incremental and historical DataFrames and return a final DataFrame</p></blockquote>
                        <blockquote><p>Load — Writes the data into the final output path</p></blockquote>
                        <blockquote><p>Run — Does the integration between ETL process. It is exposed to the job submitter module. It accepts the spark session, job configurations, and a logger object to execute the pipeline.</p></blockquote>
                        <p><strong>configs</strong> <strong>and ddl</strong> — We will take out the static configurations and place them in a JSON file (configs/config.json) so that it can be overwritten as per the test config. </p>
                        <script src="https://gist.github.com/soyelherein/20a0ea4114d782fa85e39d13cb82a3bf.js"></script>
                            <script src="https://gist.github.com/soyelherein/ab5024b4da6aa22a1b84e4b7aa6eaf7b.js"></script>
                            <p>Given that we have structured our ETL jobs in testable modules. Last thing is to take out the spark environment management before jumping into tests.</p>
                            <h3>Decouple Spark Environment</h3>
                            <p>As it becomes tedious and impractical to test and debug spark-jobs by sending them to a cluster (<em>spark-submit</em>) and teams can become Sherlock Holmes — investigating clues in stack-traces on what could have gone wrong.<br>
                                <strong>pipenv</strong> — To avoid the lifeless scenarios we might encounter, we can create an isolated environment (say thanks to <a href='https://pypi.org/project/pipenv/'><em>pipenv</em></a>) to initiate a <em>Pyspark</em> session whereas:<br>
                                <ul>
                                    <li><p>all development and production dependencies are described in the Pipfile</p>
                                    </li>
                                    <li><p><em>pipenv</em> helps us managing project dependencies and Python environments (i.e. virtual environments)</p>
                                    </li>
                                    <li><p>convenient with dependencies management on an ad-hoc basis just with <code>pip install pipenv --dev</code></p>
                                    </li>
                                </ul>
                                </p>
                                <div class="container">
                                    <div class="row">
                                    <div class="col-md-6 col-sm-6">
                                        <p>
                                <strong>dependencies.job_submitter</strong> — Since a data application can have numerous upstream and downstream pipelines, it makes sense to take the spark environment management and other common tasks into a shared entry point so that the applications can focus only on their business logic.<br/>
                                    This submitter module takes the job name as an argument and executes the functionality defined in it. The pipeline itself has to expose a run method(discussed in the Decouple Application section) that is the entry point for the ETL.<br>
                                    This standalone module is entrusted with starting and stopping spark sessions, 
                                parsing the configuration files containing static variables(<strong>configs/config.json </strong> or mentioned otherwise), 
                                and any dynamic command-line arguments then executing the requested pipeline for all pipelines. 
                                Please head back to the <a href='https://github.com/soyelherein/pyspark-cicd-template/blob/master/dependencies/job_submitter.py'>Github</a> repo for the details.<br>
                                With this submitter module, the command to submit the pipeline becomes:<br>
                                 </p>
                               </div>
                               <div class="col-md-6 col-sm-6">
                                <iframe src="{{ '/img/blog' | relative_url }}_pyspark_cicd/handshake.gif" width="480" height="270" frameBorder="0" class="giphy-embed" allowFullScreen></iframe>
                               </div>
                            </div>
                        </div>
                        <pre><p>$SPARK_HOME/bin/spark-submit \
                                --py-files dependencies/job_submitter.py, jobs/pipeline_wo_modules.py \
                                  dependencies/job_submitter.py   --job    pipeline_wo_modules  </p>
                        </pre>          
                    <h3>Testbed</h3>
                            <p>
                                <strong>conftest</strong> — We have used <a href='https://docs.pytest.org/en/stable/'>Pytest</a> style tests for our pipeline along with leveraging a few features (i.e. mock, patch) from <a href='https://docs.python.org/3/library/unittest.html'>unittest</a>.
                                This file does the heavy lifting of setting up jobs for tests i.e. providing test sparkSession and mocks creating the tables and DataFrames locally from the CSV files. The mapping is defined in the testbed.json file.
                                <script src="https://gist.github.com/soyelherein/2ada1d422625dbed3f6da22330dccded.js"></script>
                                This config is pretty self-explanatory. We have defined the DataFrame and table details under the “data” key.<br/>
                                If the job accepts any dynamic parameter as job-args(i.e. process_date), that override should be part of the “config” key.
                                It would be sent as a dictionary argument to the job. <a href="https://github.com/soyelherein/pyspark-cicd-template/blob/286111c75714d09092df68d92c57012f3e5fc272/tests/conftest.py#L90">setup_testbed</a> a helper method is responsible for producing the DataFrame and tables once the test_bed.json file is configured.
                                The file format can be configured as per the need in the conftest, default is as shown below.
                                <script src="https://gist.github.com/soyelherein/caf452da8675ebefdf4f3d0679577463.js"></script>
                                For read and write operations we encourage teams to use the generic methods like “<a href='https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html'>read.load</a>” and “<a href='https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html'>write</a>”, instead of “read.csv” or “read.orc” so that our mocks can be more generic.</p>
                                <p><strong>test_pipeline —</strong>We have created a session-level <a href='https://docs.pytest.org/en/stable/fixture.html'>pytest fixture</a> containing all the hard work done in the conftest in an object. As you see in the later section we will perform the entire testing using its member attributes.<br/>
                                 <iframe src="{{ '/img/blog' | relative_url }}_pyspark_cicd/testing.gif" width="500" height="300" frameBorder="0" class="giphy-embed" allowFullScreen ></iframe>
                                <br>Now let’s test our <strong>transform</strong> method that takes the incremental and historical DataFrames as input and produces the final DataFrame.
                                <script src="https://gist.github.com/soyelherein/c03d865f8a16e9999e1526e70543aca7.js"></script>
                                Since the I/O operations are already been separated out we can introspect the calling behavior of <strong>extract and load</strong> using mocks. These mocks are set up in the conftest file.
                                <script src="https://gist.github.com/soyelherein/382b51acc81981bbfa91b7fde6872911.js"></script>
                                Since we have already tested individual methods we can make use of patching to do the <strong>integration</strong> test by patching the outcomes of different functions and avoiding side-effects of writing into the disk.
                                <script src="https://gist.github.com/soyelherein/bdcb5721b5ea7d0274d2be9c903d2672.js"></script>
                                These tests can be run from IDE or by simply running <code>pytest</code> command.</p>
                                Below is the configuration for PyCharm:
                                <p>
                                    <iframe src="{{ '/img/blog' | relative_url }}_pyspark_cicd/pycharm.png" width="800" height="368"  frameBorder="0" class="giphy-embed" allowFullScreen ></iframe>
                                </p>
                                And the output of pytest:
                                <p>
                                    <iframe src="{{ '/img/blog' | relative_url }}_pyspark_cicd/pytest.png" width="800" height="368"  frameBorder="0" class="giphy-embed" allowFullScreen ></iframe>
                                </p>
                                <p>In a complex production scenario, related pipeline methods can be connected in terms of inputs and expected outputs which is immutable. A fair understanding of application and segregation of different subject area can provide a valuable regression like confidence for CICD integration.</p>                    
                                <p><h3>CICD : </h3>
                                <strong>Dockerfile —</strong> Contains the dockerized container with the virtual environment set up for the Jenkins agent.<br/>
                                <strong>Makefile —</strong>  This Makefile utility zips all the code, dependencies, and config in the packages.zip file so that Jenkins can create the artifact, and the CD process can upload it into a repository. The final code can be submitted as below:<br/>
                                <pre><code class='language-shell' lang='shell'>$SPARK_HOME/bin/spark-submit \
                                --py-files packages.zip \
                                --files configs/config.json \
                                dependencies/job_submitter.py --job pipeline --conf-file configs/config.json
                                </code></pre>
                                <strong>Jenkinsfile —</strong> It defines the CICD process. where the Jenkins agent runs the docker container defined in the Dockerfile in the prepare step followed by running the test. Once the test is successful in the prepare artifact step, it uses the makefile to create a zipped artifact. The final step is to publish the artifact which is the deployment step.<br/>
                                <script src="https://gist.github.com/soyelherein/18b8ab7a25bad99e1eba20aa1f046434.js"></script></p>
                                <br/>
                                <p><iframe src="{{ '/img/blog' | relative_url }}_pyspark_cicd/jenkins.png" alt="jenkins project config img" width="682" height="400" frameBorder="0" class="giphy-embed" allowFullScreen> </iframe></p>
                                <p>
                                Source code: <a href='https://github.com/soyelherein/pyspark-cicd-template'><strong>https://github.com/soyelherein/pyspark-cicd-template</strong></a>

                                </p>


                                    
                                <h3>References</h3>
                                <p><a href='https://alexioannides.com/2019/07/28/best-practices-for-pyspark-etl-projects/' target="_empty"><strong>Best Practices for PySpark ETL Projects</strong>
                                <em>I have often lent heavily on Apache Spark and the SparkSQL APIs for operationalising any type of batch data-processing…</em>alexioannides.com</a></p>
                                <p><a href='https://developerzen.com/best-practices-writing-production-grade-pyspark-jobs-cb688ac4d20f' target="_empty"><strong>Best Practices Writing Production-Grade PySpark Jobs</strong>
                                <em>How to Structure Your PySpark Job Repository and Code</em>developerzen.com</a></p>
                                <p><a href='https://medium.com/wbaa/datas-inferno-7-circles-of-data-testing-hell-with-airflow-cef4adff58d8' target="_empty"><strong>Data’s Inferno: 7 Circles of Data Testing Hell with Airflow</strong>
                                <em>Why data testing is hard, but you should really do it!</em>medium.com</a></p>

                                <p style="text-align: right; color:gray">Published on 18th December 2020 ©soyelherein.github.io</p>
                 </div>
                 <div class="container">
                    <div class="row">
                        <div class="col-md-1 col-sm-1">
                 <!-- LikeBtn.com BEGIN -->
<span class="likebtn-wrapper" data-theme="github" data-ef_voting="buzz" data-rich_snippet="true" data-dislike_enabled="false" data-voting_cancelable="false" data-voting_frequency="60" data-group_identifier="soyelherein" data-counter_frmt="km" data-share_size="large"></span>
<script>(function(d,e,s){if(d.getElementById("likebtn_wjs"))return;a=d.createElement(e);m=d.getElementsByTagName(e)[0];a.async=1;a.id="likebtn_wjs";a.src=s;m.parentNode.insertBefore(a, m)})(document,"script","//w.likebtn.com/js/w/widget.js");</script>
<!-- LikeBtn.com END -->
</div>
<div class="col-md-1 col-sm-1">
                 <!-- begin wwww.htmlcommentbox.com -->
                    <script type="IN/Share" data-url="https://soyelherein.tech/blog/pyspark-cicd.html"></script>
                </div>
            </div>
                </div>
            
 <div id="HCB_comment_box"><a href="http://www.htmlcommentbox.com">HTML Comment Box</a> is loading comments...</div>
 <link rel="stylesheet" type="text/css" href="https://www.htmlcommentbox.com/static/skins/bootstrap/twitter-bootstrap.css?v=0" />
 <script type="text/javascript" id="hcb"> /*<!--*/ if(!window.hcb_user){hcb_user={};} (function(){var s=document.createElement("script"), l=hcb_user.PAGE || (""+window.location).replace(/'/g,"%27"), h="https://www.htmlcommentbox.com";s.setAttribute("type","text/javascript");s.setAttribute("src", h+"/jread?page="+encodeURIComponent(l).replace("+","%2B")+"&mod=%241%24wq1rdBcg%2492B66tEYAEai1DwnXOiuL0"+"&opts=16862&num=10&ts=1608774727642");if (typeof s!="undefined") document.getElementsByTagName("head")[0].appendChild(s);})(); /*-->*/ </script>
<!-- end www.htmlcommentbox.com -->
          </div>
  </section>



    <!-- Global javascript -->
    <script src="{{ '/js/jquery/jquery.min.js' | relative_url }}"></script>
    <script src="{{ '/js/bootstrap/bootstrap.bundle.min.js' | relative_url }}"></script>
    <script src="{{ '/js/jquery-easing/jquery.easing.min.js' | relative_url }}"></script>
    <script src="{{ '/js/counter/jquery.waypoints.min.js' | relative_url }}"></script>
    <script src="{{ '/js/counter/jquery.counterup.min.js' | relative_url }}"></script>
    <script>
        $(document).ready(function(){

        $(".filter-b").click(function(){
            var value = $(this).attr('data-filter');
            if(value == "all")
            {
                $('.filter').show('1000');
            }
            else
            {
                $(".filter").not('.'+value).hide('3000');
                $('.filter').filter('.'+value).show('3000');
            }
        });

        if ($(".filter-b").removeClass("active")) {
          $(this).removeClass("active");
        }
        $(this).addClass("active");
        });

        // SKILLS
        $(function () {
            $('.counter').counterUp({
                delay: 10,
                time: 2000
            });

        });
    </script>
</body>

</html>
